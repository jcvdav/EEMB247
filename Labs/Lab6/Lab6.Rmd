---
title: 'EEMB247/BMSE247: Computer lab 6: Stochastic Models'
author: "Cherie Briggs"
date: "February 23, 2018"
header-includes:
   - \usepackage{amsmath}
output:
  html_document: default
---

******

##Part 1:  Brief tutorial on probability distributions in R

####Several useful distributions to know about for modeling are:  

####Discrete Distributions (can take on only discrete values)

 **Distribution (`R name`)**  

* Binomial  (`binom`)
* Poisson (`pois`)
* Negative Binomial (`nbinom`)
* Geometric (`geom`)
* Beta Binomial (`bbinom`) 

#### Continuous Distributions (can take on any value, or alternatively, any value within some range)

 **Distribution (`R name`)**  
 
* Uniform (`unif`)
* Normal (`norm`)
* Gamma (`gamma`)
* Exponential (`exp`)
* Beta (`beta`)
* Lognormal (`lnorm`)

#### There are also a number of other distributions that you come across in statistical tests (that we won't discuss here):
* Student t (`t`)
* F (`f`)
* Chisquare (`chisq`)
* Tukey (`tukey`)
* Wilcoxon (`wilcox`)
* Logistic (`logis`)

*****

### Distributions in R:
For any of these distributions in R, there are 4 functions (where *name* is the R name of the any of the distributions, above):

 function name | description
------------- | -------------
**d***name*( )   | density or probability function
**p***name*( )   | cumulative density
**q***name*( )   | quantile function
**r***name*( )   | random deviates

We will illustrate the use of these with the familiar Normal distribution, which has the R name `norm`.

*****

#### Density or Probability Function, `dname( )`  
A probability distribution is the set of probabilities on a sample space or set of outcomes. A **discrete distribution** can be described by its **distribution function**, which is just a formula for the probability that the outcome of an experiment or observation (called a **random variable**) $X$ is equal to a particular value $x$.  That is, $f(x) = \text{Prob}\left(X = x\right)$. 

For a **continuous distribution**, such as the normal distribution, $x$ can take on an infinite number of values, so the probability of any specific value is vanishingly small.  Continuous probability distributions, such as the normal distribution, are expressed as **probability densities** rather than probabilities, that is, the probability that random variable $X$ is between $x$ and $x + \Delta x$, divided by $\Delta x$. Dividing by $\Delta x$ allows the observed probability density to have a well-defined limit as $\Delta x$ shrinks to zero. 

The probability density function for a normal distribution with mean= $\mu$ and standard deviation = $\sigma$ is:
$$
f(x|\mu,\sigma) = \frac{1}{\sigma\sqrt{2\pi}}\exp^{-\frac{\left(x-\mu\right)^2}{2\sigma^2}}
$$

The R command to get the density function for the normal distribution is:  
`dnorm(x, mean = 0, sd = 1, log = FALSE)`  
where `x` is a vector of values for which you want the density function.  
`mean` is a vector of the mean(s).  If not specified, the default value of zero is used.  
`sd` is a vector of the standard deviation(s).  If not specified, the default value of 1 is used.  
`log` logical (TRUE or FALSE).  If TRUE, the probabilities are given as log(p).  

For example:
```{r}
# This plots the density function for a normal distribution with mean=0, and sd=1
x <- seq(-5,5,0.01)
f <- dnorm(x)
plot(x,f, type = 'l', lwd=2, col = 'blue',main="probability density for N(0,1)")
```
```{r}
# This gives the probability density function for x = 0, for 10 different values of the mean
# It's not immediately obvious why you might want to do this, but who knows...
x <- rep(0,10)
mu <-seq(1,10)
y<-dnorm(x,mean=mu,sd = 1)
y
```

*****

### Cumulative Density Function, `pname( )`
A distribution can also be described by its cumulative distribution function $F(x)$ (note the uppercase $F$), which is the probability that the random variable $X$ is less than or equal to a particular value of $x$: $(F(x) = \text{Prob}(X \leq x))$.  This means that the cumulative distribution function is the integral (or the sum for a discrete distribution) of the probability density function from $-\infty$ to $x$:
$$
F(x) = \int_{-\infty}^{x} f(x) dx
$$

Because the integral of the probability density function from $-\infty$ to $\infty$ is 1, this means that the cumulative distribution function at $x =\infty$ is 1.  Cumulative distribution functions are useful for calculating the probability below given values of $x$, between given values of $x$, or above given values of $x$.

The R command to get the cumulative density function for the normal distribution is:  
`pnorm(x, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)`
 
where `x` is a vector of values for which you want the cumulative density function.  
`mean` is a vector of the mean(s).  If not specified, the default value of zero is used.  
`sd` is a vector of the standard deviation(s).  If not specified, the default value of 1 is used.  
`lower.tail` logical (TRUE or FALSE); if TRUE (default), probabilities are P[X ≤ x] otherwise, P[X > x].  
`log.p` logical (TRUE or FALSE).  If TRUE, the probabilities are given as log(p). The default value is FALSE.  

For example:

```{r}
# This plots the cumulative density function for a normal distribution with mean=0, and sd=1
x <- seq(-5,5,0.01)
F <- pnorm(x)
plot(x,F, type = 'l', lwd=2, col = 'blue',main="cumulative density for N(0,1)")
```

```{r}
# For a normal distribution with mean=0, and sd=1
# Calculate the probability of x < -2
pnorm(-2)

# Calculate the probability of x between -1 and 1:
pnorm(1) - pnorm(-1)

# Calculate the probability of x > 1:
1 - pnorm(1)
#or, alternatively
pnorm(1, lower.tail = FALSE)
```

*****

### Quantile Function, `qname( )`
The quantile function is inverse of the cumulative distribution function.  It will tell you the value of $x$ at which a given quantile of the distribution occurs.  That is, if we want to know the value of $x$ from a standard normal distribution (with mean=0 and sd=1) at which 90% of probability occurs at values less than $x$, we can use the function `qnorm(0.9)`, which will return `1.281552`.  This means that `1.281552` is the 90% quantile of the standard normal distribution. 

The R command for the quantile function for the normal distribution is:  
`qnorm(p, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)`
 
where `p` is a vector of probabilities.  
`mean` is a vector of the mean(s).  If not specified, the default value of zero is used.  
`sd` is a vector of the standard deviation(s).  If not specified, the default value of 1 is used.  
`lower.tail` logical (TRUE or FALSE); if TRUE (default), probabilities are P[X ≤ x] otherwise, P[X > x].  
`log.p` logical (TRUE or FALSE).  If TRUE, the probabilities are given as log(p). The default value is FALSE.  

For example, if we want to know the values of $x$ at which the 5%, 10%, 15%, ..., 90%, and 95% quantiles occur for the standard noormal distribution, we could use the following commands:

```{r}
p <- seq(0.05,0.95,by=0.05)
x <-qnorm(p)
x
```

*****

### Random Deviates, `rname( )`
Random deviates is just another way to say random draws from a particular probability distribution. Being able to take random draws from any distribution is incredibly useful for stochastic modeling.  

The R command for the quantile function for the normal distribution is:  
`rnorm(n, mean = 0, sd = 1)`
 
where `n` is the number of draws.  
`mean` is a vector of the mean(s).  If not specified, the default value of zero is used.  
`sd` is a vector of the standard deviation(s).  If not specified, the default value of 1 is used.  
for example:
```{r}
#take 5 random draws from a normal distribution with mean = 10, and sd = 2
rnorm(5,mean=10,sd=2)
```

for example:
```{r}
#take a sample of 100 random draws from a normal distribution with mean = 10, and sd = 2
mysample<-rnorm(100,mean=10,sd=2)

#plot them as a histogram
hist(mysample, col="blue",breaks=seq(0.5,20.5),freq=FALSE)

#superimpose on the histogram the probability density function
x<-seq(0,20,0.1)
y<-dnorm(x,mean=10,sd=2)
lines(x,y,col="red",lwd=2)

```
```{r}
#increase the size of the sample to 10000
mysample<-rnorm(10000,mean=10,sd=2)

#plot them as a histogram
hist(mysample, col="blue",breaks=seq(0.5,20.5),freq=FALSE)

#superimpose on the histogram the probability density function
x<-seq(0,20,0.1)
y<-dnorm(x,mean=10,sd=2)
lines(x,y,col="red",lwd=2)

```

*****

## Flipping a coin in R:
In the exercise below, we'll need to know how to tell R to flip a (potentially biased) coin, or roll a (poentially biased) n-sided die, and do different things depending on the outcome.

In R, the useful command for this situation is `runif(1)`, which generates a single draw from a **uniform distribution** between 0 and 1.

If you want to flip an unbiased coin, such that the probability of heads is $p=0.5$, then you can say `u=runif(1)`, and if $u < 0.5$ then the outcome is heads and if $u \geq 0.5$, then it's tails.

If you want to flip a biased coin, such that the probability of heads is $p=0.55$, then the criteria for a heads is $u < 0.55$, and the criteria for tails is u \geq 0.55$.

If we are using a biased coin for which the probability of heads is $p=0.55$, and we want to set$x = 1$ if the outcome is heads, and $x = 2$ if the outcome is tails, then the R code is:

```{r}
p=0.55
u=runif(1)
if (u<p) {
  x <- 1
} else {
	x <- 2
}
x
```
We could also do this using the handy-dandy ifelse command:
```{r}
x <- ifelse(runif(1)<0.55,1,2)
x
```

*****

## Rolling the dice in R:
Now imagine you have a 3-sided die (hmm, that’s not actually possible.  Maybe it’s actually a 3-sided spinner), with $p_1 = 0.2$, $p_2 = 0.3$, and $p_3 = 1-p_1-p_2$.  Draw a uniformly distributed random number between 0 and 1: `u=runif(1)`. If $u < p_1$, then you rolled a 1.  If $p1\leq u < (p_1+p_2)$, then you rolled a 2.  If $u \geq (p_1+p_2)$, then you rolled a 3.  R code:

```{r}
p1 = 0.2
p2 = 0.3
u=runif(1)
if (u<p1) {
         # <insert whatever code you want executed upon rolling a 1>
} else if (u<(p1+p2) ) {
         # <insert whatever code you want executed upon rolling a 2>
} else {
         # <insert whatever code you want executed upon rolling a 3>
}

```

**Note:**  The reason we are giving assigning a name (`u`) to the draw from the uniform distribution, rather than just referring to it as `runif(1)` is that every time we use `runif(1)`, R will generate a new draw from the uniform distribution.



*****
*****

## Part 2:  Production and decay of mRNA:  a stochastic “birth-death” process:
Consider the transcription of gene $D$ into mRNA ($M$), with reaction rate $k_1$, and then subsequent translation into protein ($P$), with reaction rate $k_2$.  mRNA and proteins are then degraded through reactions, with decay rates $k_3$ and $k_4$ (which means the half-lives are: $t_M = ln(2)/k_3$ and $t_P = ln(2)/k_4$).

$$
\begin{align}
   D\hspace{3mm} &  \xrightarrow{\hspace{3mm} k_1 \hspace{3mm}}& D + M\\
\\
   M \hspace{3mm}& \xrightarrow{\hspace{3mm}  k_2 \hspace{3mm} } & M + P\\
\\
   M\hspace{3mm} & \xrightarrow{\hspace{3mm}  k_3 \hspace{3mm} } & .
\\
   P\hspace{3mm} & \xrightarrow{\hspace{3mm}  k_4 \hspace{3mm} } & .
\end{align}
$$
We could write this as a system of deterministic ordinary differential equations (ODEs) for the concentrations of $M$ and $P$.  Remember, if we do this, we are making (at least) 2 big assumptions:

1. We assume that we are dealing with a large number of molecules that the reactions occur as rates, rather than as discrete events. 

2. The system is well-mixed.

We will discuss how to deal with violations to the first of these 2 assumptions in this exercise.  

The ODEs for this system are:
$$
\begin{align}
& \frac{dM}{dt} = k_1D – k_3M \\
\\
& \frac{dP}{dt} = k_2M – k_4P
\end{align}
$$
If we assume that $D$ is constant, this system has a stable equilibrium at $M^* = (k_1/k_3)D$, and $P^* = (k_2/k_4)M^*$.



If instead, we assume that there are few copies of some of these molecules, we could consider the production and decay of mRNA as a **stochastic birth-death process**:  

Let $p_n(t)$ be the probability of having $n$ molecules of mRNA at time $t$. Assume $D = 1$.  The corresponding **Master equation** describes the rate of change of the probability of having $n$ molecules of mRNA at time $t$:

$$
\begin{align}
& \frac{dp_n}{dt} = k_1 \left( p_{n-1} - p_n \right) + k_3\left[ \left(n+1 \right)p_{n+1} - np_n \right] & \text{(Equation 1) } 
\end{align}
$$

The steady state solution, which satisfies $(n+1)p_{n+1} = np_n +(k_1/k_3) [ p_n  - p_{n-1}]$ can be found recursively:

$$
\begin{align}
&p_1 = \left[\frac{k_1}{k_3}\right]p_0 \\
&p_2 = \left(\frac{1}{2}\right) \left[\frac{k_1}{k_3}\right]^2 p_0\\
&\vdots \\
&p_n = \left(\frac{1}{n}\right) \left[\frac{k_1}{k_3}\right]^n p_0\\
\\
&\text{with} &p_{-1} = 0
\end{align}
$$
Normalization of this probability function (i.e. making sure that the sum of the probabilities adds up to 1) then gives $p_0 = exp(-k_1/k_3)$.  

The steady state distribution of $M$ thus follows a **Poisson distribution**, with average steady state value equal to $(k_1/k_3)$.

*****

## Computer Exercise: Stochastic simulation
For the "Production and decay of mRNA" example, the differential equations in Equation 1 describe the rate of change of the probability that the system will be in a given state at any point in time.
**How do you generate one realization of this system, from a given starting condition?**  This is the goal of this part of the Computer Exercise. 

###Method 1: Approximate stochastic realization by dividing the system into discrete time steps:
One option would be to discretize time into small time steps $\Delta t$, and use the information that starting at a given number of mRNA molecules M = i:  
the probability of increasing $M$ from $i \rightarrow i + 1$ is:  $k_1 D \Delta t$  
the probability of decreasing $M$ from $i\rightarrow i − 1$ is:   $k_3 i \Delta t$ 
the probability of staying at $M$ is:  $(1 - k_1 D \Delta t - k_3 i \Delta t)$  

####The recipe for generating a single stochastic realization using Method 1 is:
1.	Assign values to you parameters, $k_1$, $k_3$, and $D$ (you can set $D=1$), and set your time increment $\Delta t$.  (Note:  $(k_1 D \Delta t - k_3 i \Delta t)$ has to be less than or equal to 1, so if $k_1$ or $k_3$ is large, then you’ll have to use a small time step, $\Delta t$.)  

2.	Initialize $M$ to some starting value.  (If you want to save the values of $M$ at each time step, for example to plot them out at the end, then set up some type of R structure in which to hold the results (e.g. a vector or an array, or just `Mvalues<-numeric(0)`).

3.	Loop through time from a start time (e.g. 0) to a stop time in increments of $\Delta t$ (perhaps using a `for` loop).  In each iteration of the loop do the following:  
a.	calculate the probability of $M$ increasing by 1 (this actually doesn’t change through time)  
b.	calculate the probability of $M$ decreasing by 1 (this depends on the current value of $M$)  
c.	calculate the probability of $M$ staying the same  
d.	roll a 3-sided die and depending on the outcome either increase $M$, decrease $M$, or have $M$ remain unchanged.  

4.	Plot your results through time.  

*****


>###Computer Exercise Task 1:
>a)	In R, implement the recipe for generating a single stochastic realization using Method 1, and plot one realization of the production and decay of mRNA model through time.

```{r}
set.seed(43)

D <- 1
k1 <- 0.1
k3 <- 0.001
dt <- 0.1
Tend <- 10000
time <- seq(0, Tend, dt)

M <- 20

p1 <- k1*dt
p2 <- k3*M*dt
Msave <- numeric(length(time))

for(i in 1:length(time)){
  Msave[i] <- M
  
  p2 <- k3*M*dt
  
  u <- runif(1)
  
  if(u < p1){
    M <- M + 1
  } else if(u < (p1 + p2)){
    M <- M-1
  } else {
    M <- M
  }
}



plot(time, Msave, type = "l")
```


>b)	In R, set up a `for` loop to run a large number of stochastic realizations of this model, only saving the final state of the system for each run (e.g. the value of M at time = 10000).

```{r}

set.seed(43)

reps <- 1000
M_last <- numeric(reps)

for(j in 1:reps){
  M <- 20
  
  p1 <- k1*dt
  p2 <- k3*M*dt

  for(i in 1:length(time)){
    p2 <- k3*M*dt
    
    u <- runif(1)
    
    if(u < p1){
      M <- M + 1
    } else if(u < (p1 + p2)){
      M <- M-1
    } else {
      M <- M
    }
  }
  M_last[j] <- M
}

```


>Plot a histogram of these states.  They should approximate a Poisson distribution with mean = k1/k3.

```{r}
hist(M_last, freq = F, col = "grey")

x <- seq(min(M_last), max(M_last))
y <- dpois(x, (k1/k3))

lines(x,y,col="red",lwd=2)
```


>For example, if your list of final values of M for your multiple runs is stored in `Mstate`, you can plot a histogram of the frequencies using the command:
>
`hist(Mstate, freq = FALSE, col = "grey")`  
>
>You can add to this histogram the predictions from the Poisson distribution using the commands:
>`xfit<-seq(min(Mstate),max(Mstate))`  
>`yfit<-dpois(xfit,(k1/k3))`  
>`lines(xfit, yfit, col="blue", lwd=2)`  

>c)	Try altering k1 and k3 to see what this does to the predictions.  Try altering $\Delta t$ to see how this affects the fit to the Poisson distribution.

#### Altering k1 and k3

```{r}
k1 <- 1

M_last <- numeric(reps)

for(j in 1:reps){
  M <- 20
  
  p1 <- k1*dt
  p2 <- k3*M*dt

  for(i in 1:length(time)){
    p2 <- k3*M*dt
    
    u <- runif(1)
    
    if(u < p1){
      M <- M + 1
    } else if(u < (p1 + p2)){
      M <- M-1
    } else {
      M <- M
    }
  }
  M_last[j] <- M
}

png("hist2.png", width=6, height=4, units="in", res=300)
hist(M_last, freq = F, col = "grey", main = "Histogram of M_last with k1 = 1")
x <- seq(min(M_last), max(M_last))
y <- dpois(x, (k1/k3))
lines(x,y,col="red",lwd=2)
dev.off()
```

```{r}
dt <- 0.5
Tend <- 10000
time <- seq(0, Tend, dt)

M_last <- numeric(reps)

for(j in 1:reps){
  M <- 20
  
  p1 <- k1*dt
  p2 <- k3*M*dt

  for(i in 1:length(time)){
    p2 <- k3*M*dt
    
    u <- runif(1)
    
    if(u < p1){
      M <- M + 1
    } else if(u < (p1 + p2)){
      M <- M-1
    } else {
      M <- M
    }
  }
  M_last[j] <- M
}

hist(M_last, freq = F, col = "grey", main = "Histogram of M_last with dt = 1")

x <- seq(min(M_last), max(M_last))
y <- dpois(x, (k1/k3))

lines(x,y,col="red",lwd=2)
```



*****

###Method 2: Exact stochastic realization using Gillespie’s algorithm:

Although Method 1 is easy to understand and implement, it is just an approximation and assumes that $\Delta t$ is small enough that the probability that two events (births and deaths) occur within $\Delta t$ is negligible.  The Gillespie algorithm (Gillespie 1977) is an exact alternative that instead treats time as continuous and uses two random numbers to determine (i) the time to the next event and (ii) which event occurs.  

The general set-up for this Stochastic Simulation Algorithm (SSA) was first formulated for chemical systems, but it applies to ecological systems equally well (and has been used in a number of ecological models).  The set-up is as follows (from: Li and Petzold, Bioinformatics 2005):

Consider a spatially homogeneous chemically reacting system with a fixed volume and at a constant temperature.  

The system involves $N$ molecular species: $\left\{S_1, ..., S_N\right\}$, represented by the dynamical state vector $X(t) = \left(X_1(t), ..., X_N(t)\right)$, where $X_i(t)$ is the population of species $S_i$ in the system at time $t$.  

In the system being modeled, there are $K$ chemical reactions, $\left\{R_1, ..., R_K\right\}$.  Each reaction $R_j$ is characterized by:  

1. a **propensity function** $a_j$, where $a_j(x)dt$ is the probability, given the state of the system at time $t$, that one $R_j$ reaction will occur in the next infinitesimal time interval $(t,t+dt)$, and  
2. a **state change vector** $v_j = \left\{v_{1j}, ..., v_{Nj}\right\}$, in which $v_{ij}$ is the change in the number of species $S_i$ due to one $R_j$ reaction.

For any given current state of the system, $X(t)  = x_t$, the time $\tau$ to the next reaction is exponentially distributed with mean = $1/a_{tot}(x_t)$, where $a_{tot}(x_t)$ is the sum of the propensities of all of the possible reactions:
$$
a_{tot}\left(x_t\right) = \sum_{j=1}^Ka_j \left(x_t\right)
$$

**Take 1 draw from a uniformly distributed random variablein the interval $\left[0,1\right]$, and call this $u_1$**.  
At any point in time, a value of $\tau$ (time to the next reaction) can be generated by:
$$
\tau = \frac{1}{a_{tot}\left(x_t\right)}\ln\left(\frac{1}{u_1}\right)
$$

And, the probability that the next reaction is of type $j$ is: 
$$
P\left( j|x,t \right) = \frac{a_{j}\left(x_t\right)}{a_{tot}\left(x_t\right)}
$$
for each reaction $(j = 1, ...,K)$.

**Take a second draw from a uniformly distributed random variablein the interval $\left[0,1\right]$, and call this $u_2$**.  
The type of the next reaction can be determined by rolling a K-sided die, with the probability of the die landing on each of the $K$ sides being $P(j|x,t)$ for $j = 1,...,K$.

*****

####The recipe for generating a single stochastic realization using Method 2, The Gillespie Algorithm, is:

1.	Assign values to you parameters, $k_1$, $k_3$, and $D$ (you can set $D=1$).

2.	Initialize the state of the system to some starting value.  In this case we have only a single state variable $X(t) = M(t)$.  Again, if you want R to save the values of $M$ at each time step, for example to plot them out at the end, then set up some type of R structure in which to hold the results (e.g. a vector or an array, or just `Mvalues<-numeric(0)`).

3.	Initialize time to `t = 0`.
(If you want to plot your results through time, you’ll also want to save the value of time at each time step, e.g. `tvalues<-numeric(0)`)

4.	Loop through time, in this case because time is going to jump forward in uneven increments, perhaps use a `while` loop (e.g. `while (t<Tend)`, where `Tend` is whatever final time you choose for your simulations).  In each iteration of the loop do the following:  

a.	Calculate the current values of the **propensity functions** based on the current state of the system.    
  For this example, we have 2 possible reactions (we are ignoring the reactions that don’t affect the concentration of M):
$$
\begin{align}
 &R1: &  D\hspace{3mm} &  \xrightarrow{\hspace{3mm} k_1 \hspace{3mm}}& D + M\\
 \\
 &R2: &  M\hspace{3mm} & \xrightarrow{\hspace{3mm}  k_3 \hspace{3mm} } & .
\end{align}
$$

    The propensities of these two reactions are:  
    $a_1 = k_1D$  
    $a_2 = k_3M$  
    with, $a_{tot} = a_1+a_2$  

b.	Generate two random draws, $u_1$ and $u_2$, from a uniform distribution on the interval $[0,1]$.

c.	Determine the time to the next event,  $\tau = \frac{1}{a_{tot}\left(x_t\right)}\ln\left(\frac{1}{u_1}\right)$

d.	Determine which reaction occurs: if $u_2<(a_1/a_{tot})$ then reaction 1 occurs, if $u_2\geq(a_1/a_tot)$, then reaction 2 occurs.

e.	Update time: $t  = t+\tau$

f.	Update the state of the system, depending on which reaction occurs.  If reaction 1 occurs, $M$ increases by 1.  If reaction 2 occurs, $M$ decreases by 1.  

5.	Plot your results through time.

*****

>###Computer Exercise Task 2:
>a) 	In R, implement the recipe for generating a single stochastic realization using the Gillespie Algorithm, and plot one realization of the Production and decay of mRNA model through time. 

```{r}
set.seed(43)

D <- 1
k1 <- 0.1
k3 <- 0.001
dt <- 0.1
Tend <- 10000
Msave <- numeric(0)
Tsave <- numeric(0)

M <- 20

i <- 0
t <- 0

while(t<Tend){
  i <- i+1
  a1  <- k1*D
  a2 <- k3*M
  atot <- a1 + a2
  u1 <- runif(1)
  u2 <- runif(1)
  
  tau <- log(1/a1)/atot
  if(u2 < (a1/atot)){
    M <- M + 1
  } else {
      M <- M-1
  }
  t <- t + tau
  Msave[i] <- M
  Tsave[i] <- t
}

plot(Tsave, Msave, type = "l")

```


>b)	In R, set up a loop to run a large number of stochastic realizations of this model, only saving the final state of the system for each run (e.g. the value of M at time = 10000).  Plot a histogram of these states.  They should approximate a Poisson distribution with mean = k1/k3.

```{r}
set.seed(43)

reps <- 1000
M <- 20
M_last <- numeric(reps)

for(j in 1:1000){
  i <- 0
  t <- 0
  while(t<Tend){
    i <- i+1
    a1  <- k1*D
    a2 <- k3*M
    atot <- a1 + a2
    u1 <- runif(1)
    u2 <- runif(1)
    
    tau <- log(1/a1)/atot
    
    if(u2 < (a1/atot)){
      M <- M + 1
    } else {
      M <- M-1
    }
    
    t <- t + tau
  }
  M_last[j] <- M
}

hist(M_last, freq = F, col = "grey", main = "Histogram of M_last with dt = 1")

x <- seq(min(M_last), max(M_last))
y <- dpois(x, (k1/k3))

lines(x,y,col="red",lwd=2)
```


>c)	Compare the results of Method 1 and Method 2.  Does one of them appear to better approximate a Poisson distribution?

The Gillespie Algorithm can be excruciatingly slow.  For this reason a lot of research (much of it on the UCSB campus) has gone into approaches for speeding up this algorithm. 

******

******
